{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ec016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c5b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('./3dshapes.h5') :\n",
    "  print(\"file already exists\")\n",
    "else :\n",
    "    !wget https://storage.googleapis.com/3d-shapes/3dshapes.h5\n",
    "\n",
    "!ls -lh 3dshapes.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shapes3d(Dataset) :\n",
    "  def __init__(self, path) :\n",
    "    self.path = path\n",
    "    print(\"dataset to RAM\")\n",
    "    with h5py.File(path, 'r') as f :\n",
    "      self.images = f['images'][()]\n",
    "      self.labels = f['labels'][()]\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.images)\n",
    "\n",
    "  def __getitem__(self, idx) :\n",
    "    img = self.images[idx]\n",
    "    label = self.labels[idx]\n",
    "    img = torch.from_numpy(img).float()/ 255.0\n",
    "    img = img.permute(2, 0, 1)\n",
    "\n",
    "    return img, label\n",
    "\n",
    "dataset = Shapes3d('./3dshapes.h5')\n",
    "dataset_loader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe7273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "class BetaTCVAE(nn.Module) :\n",
    "  def __init__(self) :\n",
    "    super(BetaTCVAE, self).__init__()\n",
    "\n",
    "    # encoder\n",
    "    # (3, 64, 64)\n",
    "    self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "    # (32, 32, 32)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "    # (64, 16, 16)\n",
    "    self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "    # (128, 8, 8)\n",
    "    self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "    # (256, 4, ,4)\n",
    "\n",
    "    self.hl1 = nn.Linear(256 * 4 * 4, 256)\n",
    "    self.hl2_mu = nn.Linear(256, 12) # mean\n",
    "    self.hl2_logvar = nn.Linear(256, 12) # log(variance)\n",
    "\n",
    "    #decoder\n",
    "    self.hl3 = nn.Linear(12, 256)\n",
    "    self.hl4 = nn.Linear(256, 256 * 4 * 4)\n",
    "\n",
    "    self.convT1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "    self.convT2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "    self.convT3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "    self.convT4 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "  def encode(self, x) :\n",
    "    c1 = F.leaky_relu(self.conv1(x))\n",
    "    c2 = F.leaky_relu(self.conv2(c1))\n",
    "    c3 = F.leaky_relu(self.conv3(c2))\n",
    "    c4 = F.leaky_relu(self.conv4(c3))\n",
    "    h1 = F.leaky_relu(self.hl1(c4.view(-1, 256 * 4 * 4)))\n",
    "    mu = self.hl2_mu(h1)\n",
    "    logvar = self.hl2_logvar(h1)\n",
    "    logvar = torch.clamp(logvar, min=-6.0, max=6.0)\n",
    "    return mu, logvar\n",
    "\n",
    "  def reparameterize(self, mu, logvar) :\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "  def decode(self, z) :\n",
    "    h3 = F.leaky_relu(self.hl3(z))\n",
    "    h4 = F.leaky_relu(self.hl4(h3))\n",
    "    cT1 = F.leaky_relu(self.convT1(h4.view(-1, 256, 4, 4)))\n",
    "    cT2 = F.leaky_relu(self.convT2(cT1))\n",
    "    cT3 = F.leaky_relu(self.convT3(cT2))\n",
    "    cT4 = torch.sigmoid(self.convT4(cT3))\n",
    "    img = torch.clamp(cT4, min=1e-6, max=1.0)\n",
    "    return img\n",
    "\n",
    "  def forward(self, x) :\n",
    "    mu, logvar = self.encode(x)\n",
    "    z = self.reparameterize(mu, logvar)\n",
    "    recon_x = self.decode(z)\n",
    "    return recon_x, z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BetaTCVAE()\n",
    "checkpoint = torch.load(\"/kaggle/input/betatcvae-shapes3d/final_model.pth\", map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch curation\n",
    "\n",
    "def discretize(dataset) :\n",
    "    all_floats = np.array([data[1] for data in dataset])\n",
    "    all_uids = np.zeros_like(all_floats, dtype=int)\n",
    "\n",
    "    for i in range(6) :\n",
    "        _, all_uids[:, i] = np.unique(all_floats[:, i], return_inverse=True)\n",
    "\n",
    "    return all_uids\n",
    "\n",
    "class CuratedSampler :\n",
    "    def __init__(self, dataset, normal_criteria, outlier_criteria = {}) :\n",
    "        self.discrete = discretize(dataset)\n",
    "        self.dataset = dataset\n",
    "        self.normal_idx = []\n",
    "        self.outlier_idx = []\n",
    "\n",
    "        for idx in range(len(dataset)) :            \n",
    "            is_normal = True\n",
    "            for label_idx, normal_value in normal_criteria.items() :\n",
    "                if self.discrete[idx][label_idx] != normal_value :\n",
    "                    is_normal = False\n",
    "            if is_normal : self.normal_idx.append(idx)\n",
    "            else :\n",
    "                is_outlier = True\n",
    "                for label_idx, outlier_value in outlier_criteria.items() :\n",
    "                    if self.discrete[idx][label_idx] != outlier_value :\n",
    "                        is_outlier = False\n",
    "                if is_outlier : self.outlier_idx.append(idx)\n",
    "    \n",
    "    def get_batch(self, num_normal, device, num_outlier=0) :\n",
    "        normal_idxes = random.choices(self.normal_idx, k = num_normal)\n",
    "        outlier_idxes = random.choices(self.outlier_idx, k = num_outlier)\n",
    "        batch = []\n",
    "        for idx in normal_idxes :\n",
    "            batch.append(self.dataset[idx][0])\n",
    "        for idx in outlier_idxes :\n",
    "            batch.append(self.dataset[idx][0])\n",
    "            \n",
    "        return torch.stack(batch).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bad17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class VAEGradCAM :\n",
    "    def __init__(self, target_layer) :\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        target_layer.register_forward_hook(self.save_activation)\n",
    "        target_layer.register_full_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output) :\n",
    "        self.activations = output\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output) :\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def __call__(self, x, latent_idx, visualize_negative=False) :\n",
    "        mu, _ = model.encode(x)\n",
    "        target_value = mu[0, latent_idx]\n",
    "        model.zero_grad()\n",
    "\n",
    "        if visualize_negative :\n",
    "            (target_value * -1).backward()\n",
    "        else :\n",
    "            target_value.backward()\n",
    "\n",
    "        gradients = self.gradients[0]\n",
    "        activations = self.activations[0]\n",
    "\n",
    "        weights = torch.mean(gradients, dim=(1, 2))\n",
    "        cam = torch.zeros(activations.shape[1:], dtype=torch.float32).to(activations.device)\n",
    "        for i, w in enumerate(weights) :\n",
    "            cam += w * activations[i]\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        cam = cam - torch.min(cam)\n",
    "        cam = cam / (torch.max(cam) + 1e-8)\n",
    "        cam = cam.detach().cpu().numpy()\n",
    "\n",
    "        cam = cv2.resize(cam, (x.shape[3], x.shape[2]))\n",
    "\n",
    "        return cam, mu[0, latent_idx].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_cam(original, cam_map, latent_idx, latent_val, visualize_negative) :\n",
    "    img = original.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam_map), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    heatmap = heatmap[..., ::-1]\n",
    "\n",
    "    overlay = 0.5 * heatmap + 0.5 * img\n",
    "    overlay = overlay / np.max(overlay)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "    \n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(heatmap)\n",
    "    axes[1].set_title(f\"latent index {latent_idx}\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(overlay)\n",
    "    axes[2].set_title(f\"value = {latent_val:.3f}\")\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    if visualize_negative :\n",
    "        fig.suptitle(\"Heatmap of negative activations\")\n",
    "    else :\n",
    "        fig.suptitle(\"Heatmap of positive activations\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90489cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "curator = CuratedSampler(dataset, {5 : 14})\n",
    "batch = curator.get_batch(100, device)\n",
    "cam = VAEGradCAM(model.conv4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17be8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = batch[56].unsqueeze(0).to(device)\n",
    "latent_idx = 4\n",
    "heatmap1, val1 = cam(image, latent_idx=latent_idx, visualize_negative=True)\n",
    "plot_latent_cam(image, heatmap1, latent_idx, latent_val = val1, visualize_negative = True)\n",
    "heatmap2, val2 = cam(image, latent_idx=latent_idx)\n",
    "plot_latent_cam(image, heatmap2, latent_idx, latent_val = val2, visualize_negative = False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
